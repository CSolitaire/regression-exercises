{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Project: Estimating Home Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Welcome to the Ryan's Codeup Data Science MVP template! The sections in this notebook are here to help you organize your data science project into a clear workflow. The text in the markdown cells is there to help you understand the goals at each stage. Follow their instructions to move your work along, and then delete the prompts when you're done.*\n",
    "\n",
    "*Finish the mvp workflow and then go back and think critically about what you might of missed. The point isn't to doubt yourself; the purpose here is to check your blind spots and see if you can't find more information or insights that will help you to deliver better results.*\n",
    "\n",
    "*Better yet, and this cannot be stressed enough, don't ask **yourself** these questions, but bring them up in conversation with peers, experts in other fields, or even complete strangers -- anyone with a different point of view is going to be able to help you to see what things you are taking for granted.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "*What are you going to do and how are you going to do it? Write out your thoughts here in a way that you can easily explain your work to members on your team.*\n",
    "\n",
    "*This is your space to put together the elevator pitch for your project, and then follow it up with a plan of attack. It's going to be short, but that doesn't mean you won't need to spend much time putting it together -- this is a process of dropping the bad ideas until you're left with something that you are confident you can work with.*\n",
    "\n",
    "*Don't take shortcuts here; coming up with a useful question and a straightforward work plan is going to make the rest of your project flow much more smoothly from start to finish.*\n",
    "\n",
    "*One final, and very important, point: the main reason for thinking about this stage as how you're going to describe your work to others is that you should be talking about work with others!!! The more you work on your project in isolation, the better your ideas will sound to you -- even the bad ones. Listening to your ideas spoken out loud in your own voice is an excellent sanity check, and feedback from your peers is your most valuable resource.*\n",
    "\n",
    "**You don't have to answer every question, but answer each that you can now and then come back later. Also, if the question doesn't apply, remove it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "*Think about the problem you are trying to solve as a business case. After you've built your model, who is going to use it and what will they do with it? Who are your customers and stakeholders and what is their need? The need is not the data they are trying to understand -- think of the need as a specific action they would like to take or a question they want to answer.*\n",
    "\n",
    "*What is your model going to **do** in the real world? If you can answer this, and if you keep that answer in mind as you complete your work, you will have a much clearer view of where to go at each step.*\n",
    "\n",
    "* Who are your customers?\n",
    "* What is the problem?\n",
    "* What solution do you propose?\n",
    "* How will you know if your work is good?\n",
    "\n",
    "*Also, think critacally here. If someone was paying you to do this work, what would they want you to build.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stakeholders**\n",
    "Zillow Data Science Team (We are Jr. Data Science Team Members)\n",
    "\n",
    "**Problem:**\n",
    "Need a way to predict the values of single unit properties between the months of May and June 2017.\n",
    "    - What are the drivers of single unit property values (Model Features)\n",
    "    - How do you know that these drivers have signficiance (Significance Tests)\n",
    "  \n",
    "**Deliverables**\n",
    "1. Regression model built on property data between May and June 2017\n",
    "    - Can **NOT** use Tax Amount in Model\n",
    "    \n",
    "    \n",
    "2. Data Frame that Contains properties by County and State\n",
    "\n",
    "\n",
    "3. Distribution of Tax Rates per County (Min/Median/Max) histogram as visual ?\n",
    "    - Tax Amount\n",
    "    - Tax Value \n",
    "    \n",
    "    \n",
    "4. Presentation (To Board of Directors)\n",
    "    - 3-5 Slides\n",
    "      -Intro\n",
    "      -3 Body\n",
    "        - Visuals (2-3)\n",
    "      -Conclusion\n",
    "      \n",
    "      \n",
    "5. Jupyter Notebook:\n",
    "   -Data Science Pipeline:\n",
    "      - Plan \n",
    "      - Acquire\n",
    "      - Prepare\n",
    "        - Data Manipulation / Feature Engeneering\n",
    "        - Split\n",
    "        - Scale\n",
    "        - Document Process / Takaway\n",
    "      - Explore (Split)\n",
    "        - Visualizations\n",
    "          - All combinations of variables in some way.....\n",
    "          - What independent variables are correlated with the dependents\n",
    "          - What independent variables are correlated with other independent variables\n",
    "          - Check distribution of target variable (y) **Key in Model Selection**\n",
    "        - Stats Test (1 Manditory but 2 suggested)\n",
    "          - T-Test\n",
    "          - R Squared Correlation\n",
    "        - Variable Selection\n",
    "        - Document Takeaways\n",
    "          - Good Summary (Emphasized in Project Outline)\n",
    "      - Model (Scaled)\n",
    "        - Model that beats Baseline and contains 3 features (MVP) - Generalized Linear Model\n",
    "          - square feet of home (Most likely will need to calcluate w/Feature Engineering)\n",
    "          - number of bedrooms\n",
    "          - number of bathrooms\n",
    "        - Document Takeaways\n",
    "          - Which features should be included in your model?\n",
    "          - Are there new features you could create based on existing features that might be helpful?\n",
    "          - Are there any features that aren't adding much value?\n",
    "      - Delivery\n",
    "          - Plot root mean square error for each model and baseline for presentation\n",
    "            - Plot Validate\n",
    "            - Plot Test\n",
    "            - Plot Baseline\n",
    "            - Conclusion is average preformance of test and validate\n",
    "      \n",
    "      \n",
    "6. README.md\n",
    "  - Data Dictionary\n",
    "    - Explain features, and why some were chosen\n",
    "    - Details about data prep\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Plan\n",
    "\n",
    "*Now that you have a clear idea of what you are trying to accomplish, you need to find a data source. Think of this as working through a maze forwards and backwards at the same time. At the start you have any number of data sets available to work with (and the whole Internet to search and scrape), and at the end is the hypothetical data set that would answer your question immediately if you had it.*\n",
    "\n",
    "* What data, if you had it, would solve your problem right away?\n",
    "* What data do you have access to?\n",
    "* What additional data would be good to have?\n",
    "* What data would be impossible to collect?\n",
    "* What are the best proxies you can find for unavailable or impossible data?\n",
    "* What are the legal or ethical issues you might run into if you were to try to collect all of the types of data you would like to work with?\n",
    "\n",
    "*Of course you need to make a plan to turn your data into the solutions you need. Think of what type of problem this is, what models are commonly used for those types of problems, what types of data those models require and special considerations that may need to be made. Do your homework and find out what approaches other people are using on similar tasks.*\n",
    "\n",
    "* What ML paradigm are you working in? (Classification, Regression, Clustering, etc)\n",
    "* What models are commonly used in this task?\n",
    "* What other solutions are being tried in this field?\n",
    "* What special considerations need to be taken when dealing with these models? (i.e., imbalanced classes, text preprocessing, data leakage, etc)\n",
    "* How will you know that your models work?\n",
    "* How will you recognize and diagnose cases where the predictions are incorrect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Planning:**\n",
    " - Sat (Notebook Setup)\n",
    "   - acquire.py\n",
    "     - Import from SQL single unit houses between May and June 2007\n",
    "     - Wrote SQL Querry for Count and Tax Questions but they are not yet in notebook\n",
    "   - prepare\n",
    "     - Code from class (Maggie) sets up df, x_train_explore, x_train_model(Scaled)\n",
    "   - explore \n",
    "     - Plot Variable Pairs()\n",
    "     - plot_categorical_and_continuous_vars()\n",
    "   - model \n",
    "     - select_kbest()\n",
    "     - select_rfe()\n",
    "     - eval_linear_model()\n",
    "   - mvp.ipynb\n",
    "     - Outline\n",
    "     \n",
    "     \n",
    " - Sun (Data Prepareation) - Goal is a Clean Data Set For Exploration Monday\n",
    "   - Explore Data\n",
    "     - Identify Featues for Feature Engeneering\n",
    "       - Featues to Add\n",
    "       - Featues of Remove\n",
    "       - Null Values\n",
    "       - Data Validation\n",
    "       \n",
    "       \n",
    " - Mon (README / MVP)\n",
    "   - Create Explore Dataset\n",
    "     - Run Tests\n",
    "   - MVP\n",
    "   - README.md\n",
    " \n",
    " \n",
    " - Tue (Model / Round 2)\n",
    " \n",
    " \n",
    " - Wed (Edits / Project \n",
    " \n",
    "   - Goal is to work only 1/2 Day\n",
    " \n",
    " - Thur\n",
    "   - Presentations Due (Only Practice On Delivery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "*Keep all your imports in one place. This will make it much easier to see everything that you are using*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.acquire\n",
    "import src.prepare\n",
    "import src.explore\n",
    "import src.model\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire the Data\n",
    "\n",
    "### Data Pipeline\n",
    "\n",
    "*This is where you describe the ETL (Extract, Transform, Load) process. Describe the actions you will need to take to obtain the data from where it is stored, and convert it into a clean format. Also be aware of how the available data is likely to change over time -- how often will it be updated, and how long will the available data be relevant?*\n",
    "\n",
    "*After completing this step, be sure to edit the README data dictionary to include descriptions of where you obtained your data and what information it contains.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your Pipeline Tools\n",
    "\n",
    "*Your work is going to be far less useful if anyone who wants to use it or build on it is going to have to put together a new data set from scratch. Automate everything that you possibly can. (And if you don't think it can be automated, ask your peers and check Google to see what's out there.)*\n",
    "\n",
    "*The pipeline tools will connect to data sources, create local directories to store the data, download the data, clean it, reformat it, and store it. In the following step, \"Run the Pipeline,\" you'll write a function that uses all of these tools to build your data set in one line of code.*\n",
    "\n",
    "*Make sure these steps are reproducible by code. Put some thought into the directory structures and filepaths you are using to save your data, so it's easy to load files you need.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write functions that form a data pipeline from your source to data/raw\n",
    "# Once that is done move the functions to acquire.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prove it Works\n",
    "\n",
    "*Once your tools are built (and are working reliably!!), you'll be able to work much faster if they're out of sight. Collaborators and data scientists who build on your work will also be much more productive if you provide easy access to the data. So do yourself and your team a favor, and package your pipeline tools into a single function that can easily reproduce your working data set. This is what our run function in acquire is for.*\n",
    "\n",
    "*Also a helpful thing to remember is to set this function up so that when you re-run the pipeline, it checks whether the files are already available locally. This will save you lots of time downloading large files.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Running models...\n",
      "Model: Completed!\n"
     ]
    }
   ],
   "source": [
    "src.acquire.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "\n",
    "*This is the important phase and it is seen as a chore by some, even though I personally enjoy the puzzle that this stage always provides. If you tend to think of data scrubbing as a drag on your workflow, it's well worth your time to explore some new pandas functions you're unfamiliar with and speed up your workflow. Try to learn one or two new tricks with each project you work on.*\n",
    "\n",
    "*The primary scientific component to scrubbing data is going to be the thought process around deciding what to do with missing data. There are frequently no right or wrong answers for what to do with missing values, so be sure to record your thoughts, and share your process with peers or collaborators to get a sanity check.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Running models...\n",
      "Model: Completed!\n"
     ]
    }
   ],
   "source": [
    "src.prepare.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "**What Data do we need in our initial SQL querry?**\n",
    "\n",
    "Properties in the Database: Based on the above definition some categories do not fit brief (Single Unit Home)\n",
    "\n",
    "Propertylandusetypeid | propertylandusedesc\n",
    "\n",
    "-  **No**        31           Commercial/Office/Residential Mixed Used  (not a residence)\n",
    "-  **No**        46           Multi-Story Store                         (not a residence)\n",
    "-  **No**        47           Store/Office (Mixed Use)                  (not a residence)\n",
    "-            246          Duplex (2 Units, Any Combination)\n",
    "-            247          Triplex (3 Units, Any Combination)\n",
    "-            248          Quadruplex (4 Units, Any Combination)\n",
    "-            260          Residential General\n",
    "-            261          Single Family Residential\n",
    "-            262          Rural Residence\n",
    "-            263          Mobile Home\n",
    "-            264          Townhouse\n",
    "-            265          Cluster Home\n",
    "-            266          Condominium\n",
    "-  **No**        267          Cooperative                               (become shareholder not owner)\n",
    "-            268          Row House\n",
    "-            269          Planned Unit Development\n",
    "-  **No**        270          Residential Common Area                   (propterty feature)\n",
    "-  **No**        271          Timeshare                                 (become shareholder not owner)\n",
    "-            273          Bungalow\n",
    "-            274          Zero Lot Line\n",
    "-            275          Manufactured, Modular, Prefabricated Homes\n",
    "-            276          Patio Home\n",
    "-            279          Inferred Single Family Residential\n",
    "-  **No**        290          Vacant Land - General                     (not a residence)\n",
    "-  **No**        291          Residential Vacant Land                   (not a residence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "**Takeaway:**\n",
    "\n",
    "propertylandusetypeid # 31, 46, 47, 267, 270, 271, 290, 291 were removed from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Data do we need in our inital df?** (20,931 Housing Units)\n",
    "\n",
    "-    parcelid                      20931 non-null  int64          **Listing Number - Drop for Explore**\n",
    "-    id                            20931 non-null  int64          **Listing ID - Drop for Explore**\n",
    "-    airconditioningtypeid         6779 non-null   float64        **Too Many null-values - Drop for Explore**\n",
    "-    architecturalstyletypeid      52 non-null     float64        **Too Many null-values - Drop for Explore** \n",
    "-    basementsqft                  16 non-null     float64        **16 non-values - Drop for Explore** \n",
    "-    bathroomcnt                   20931 non-null  float64        Use\n",
    "-    bedroomcnt                    20931 non-null  float64        Use - Combine bath/bed (feature engeneering)\n",
    "-    buildingclasstypeid           0 non-null      object         **All Values Null - Drop for Explore**\n",
    "-    buildingqualitytypeid         13257 non-null  float64        **Too Many null-values - Drop for Explore**\n",
    "-    calculatedbathnbr             20771 non-null  float64        **Repeat of barthromcnt - Drop for Explore**\n",
    "-    decktypeid                    174 non-null    float64        **Too Many null-values - Drop for Explore**\n",
    "-    finishedfloor1squarefeet      1738 non-null   float64        **Repeat Column - Drop for Explore**\n",
    "-    calculatedfinishedsquarefeet  20868 non-null  float64        Use - Drop null\n",
    "-    finishedsquarefeet12          20024 non-null  float64        **Repeat Column - Drop for Explore**\n",
    "-    finishedsquarefeet13          17 non-null     float64        **Repeat Column - Drop for Explore**\n",
    "-    finishedsquarefeet15          736 non-null    float64        **Repeat Column - Drop for Explore**\n",
    "-    finishedsquarefeet50          1738 non-null   float64        **Repeat Column - Drop for Explore**\n",
    "-    finishedsquarefeet6           91 non-null     float64        **Repeat Column - Drop for Explore**\n",
    "-    fips                          20931 non-null  float64        **Repeat Column - Drop for Explore**\n",
    "-    fireplacecnt                  2422 non-null   float64        Use - change null to 0\n",
    "-    fullbathcnt                   20771 non-null  float64        **Repeat of bathroom - Drop for Explore**\n",
    "-    garagecarcnt                  7075 non-null   float64        Use - Rename as garage, change null to 0\n",
    "-    garagetotalsqft               7075 non-null   float64            - garagesqft verifys that they exist\n",
    "-    hashottuborspa                461 non-null    float64        Use - change null to 0, for no ht or spa\n",
    "-    heatingorsystemtypeid         13285 non-null  float64        **Too Many null-values - Drop for Explore**\n",
    "-    latitude                      20931 non-null  float64        Use\n",
    "-    longitude                     20931 non-null  float64        Use\n",
    "-    lotsizesquarefeet             18742 non-null  float64        Use - change null to 0, for no lot size\n",
    "-    poolcnt                       4496 non-null   float64        Use - change null to 0, for no pool\n",
    "-    poolsizesum                   251 non-null    float64        **Repeat Column - Drop for Explore**\n",
    "-    pooltypeid10                  121 non-null    float64        **Repeat Column - Drop for Explore**\n",
    "-    pooltypeid2                   340 non-null    float64        **Repeat Column - Drop for Explore**\n",
    "-    pooltypeid7                   4154 non-null   float64        **Repeat Column - Drop for Explore**\n",
    "-    propertycountylandusecode     20931 non-null  object         **Repeat Column - Drop for Explore**\n",
    "-    propertylandusetypeid         20931 non-null  float64        Use - Already filtered in SQL\n",
    "-    propertyzoningdesc            13437 non-null  object         **Too Many null-values - Drop for Explore**\n",
    "-    rawcensustractandblock        20931 non-null  float64        **Repeat info(zip)\n",
    "-    regionidcity                  20503 non-null  float64        **Repeat info(zip) - Drop for Explore**\n",
    "-    regionidcounty                20931 non-null  float64        **Repeat info(zip) - Drop for Explore**\n",
    "-    regionidneighborhood          8443 non-null   float64        **Too Many null-values - Drop for Explore**\n",
    "-    regionidzip                   20916 non-null  float64        Use\n",
    "-    roomcnt                       20931 non-null  float64        Use\n",
    "-    storytypeid                   16 non-null     float64        **Too Many null-values - Drop for Explore**\n",
    "-    threequarterbathnbr           2800 non-null   float64        **Repeat info(bathroom) - Drop for Explore**\n",
    "-    typeconstructiontypeid        56 non-null     float64        **Too Many null-values - Drop for Explore**\n",
    "-    unitcnt                       13476 non-null  float64        **Repeat info() - Drop for Explore**\n",
    "-    yardbuildingsqft17            701 non-null    float64        **Too Many null-values - Drop for Explore**\n",
    "-    yardbuildingsqft26            25 non-null     float64        **Too Many null-values - Drop for Explore**\n",
    "-    yearbuilt                     20850 non-null  float64        Use - Drop null values\n",
    "-    numberofstories               4917 non-null   float64        **Too Many null-values - Drop for Explore**\n",
    "-    fireplaceflag                 51 non-null     float64        **Repeat info(firepls) - Drop for Explore**\n",
    "-    structuretaxvaluedollarcnt    20897 non-null  float64        **Correlates w/Target - Drop for Explore**\n",
    "-    taxvaluedollarcnt             20930 non-null  float64        **Target Variable**\n",
    "-    assessmentyear                20931 non-null  float64        **Filtered in SQL - Drop for Explore**\n",
    "-    landtaxvaluedollarcnt         20930 non-null  float64        **Correlates w/Target - Drop for Explore**\n",
    "-    taxamount                     20931 non-null  float64        **Correlates w/Target - Drop for Explore**\n",
    "-    taxdelinquencyflag            703 non-null    object         **Correlates w/Target - Drop for Explore**\n",
    "-    taxdelinquencyyear            703 non-null    float64        **Correlates w/Target - Drop for Explore**\n",
    "-    censustractandblock           20852 non-null  float64        **Repeat Column - Drop for Explore**        \n",
    "-    id                            20931 non-null  int64          **Repeat Column - Drop for Explore**\n",
    "-    logerror                      20931 non-null  float64        **Calculation - Drop for Explore**\n",
    "-    transactiondate               20931 non-null  object         **Filtered in SQL - Drop for Explore**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Takeaway:**\n",
    " \n",
    "1. MVP \n",
    "   - Target: \n",
    "     - landtaxvaluedollarcnt\n",
    "   - Features:(3)\n",
    "     - calculatedfinishedsquarefeet\n",
    "     - bedroomcnt\n",
    "     - bathroomcnt \n",
    "\n",
    "2. 2nd Iteration Model\n",
    "    - Target: \n",
    "      - landtaxvaluedollarcnt\n",
    "    - Features:(13)\n",
    "      - bedbathratio - Combination of (bathroomcnt and bedroomcnt)\n",
    "      - calculatedfinishedsquarefeet \n",
    "      - garagecarcnt \n",
    "      - hashottuborspa\n",
    "      - latitude\n",
    "      - longitude\n",
    "      - lotsizesquarefeet\n",
    "      - poolcnt\n",
    "      - propertylandusetypeid \n",
    "      - regionidzip\n",
    "      - roomcnt\n",
    "      - yearbuilt\n",
    "      - fireplaceflag\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load (and Split) the Data\n",
    "\n",
    "*You may already have your data loaded in this notebook after the Acquire stage has been run. But you probably don't want to have to re-run your data gathering pipeline every time you try a new idea for your model. Write some code here to load your cached data set so you won't have to start from scratch.*\n",
    "\n",
    "*It is especially important to go back to a clean data set with each iteration that you want to test or add to your model. Your new results will not be reproducible if this iteration is developed on data that has been transformed in any way during the previous iteration.*\n",
    "\n",
    "*Lastly if possible split the data at this stage. It is always best to split the data as early as possible so that you are making you decision of preparation on a subset of the whole dataset as the model you are building will be doing on future unseen data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = src.acquire.get_zillow_data(True)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp \n",
    "import os\n",
    "from env import host, user, password\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer, RobustScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# New Feature (Ratio of bedroomcnt and bathroomcnt)\n",
    "df['bedbathratio'] = round(df['bedroomcnt'] / df['bathroomcnt'],2)\n",
    "\n",
    "# Rename columns for clarity\n",
    "df.rename(columns={\"hashottuborspa\":\"hottub_spa\",\"fireplacecnt\":\"fireplace\",\"garagecarcnt\":\"garage\"}, inplace = True)\n",
    "\n",
    "# Replaces NaN values with 0\n",
    "df['garage'] = df['garage'].replace(np.nan, 0)\n",
    "df['hottub_spa'] = df['hottub_spa'].replace(np.nan, 0)\n",
    "df['lotsizesquarefeet'] = df['lotsizesquarefeet'].replace(np.nan, 0)\n",
    "df['poolcnt'] = df['poolcnt'].replace(np.nan, 0)\n",
    "df['fireplace'] = df['fireplace'].replace(np.nan, 0)\n",
    "\n",
    "## Add dummy variables as new columns in dataframe and rename them, delete origional\n",
    "df[\"zip\"] = df[\"regionidzip\"].astype('category')\n",
    "df[\"zip\"] = df[\"zip\"].cat.codes\n",
    "\n",
    "### All work so far ###\n",
    "df.drop(columns= ['parcelid','id','airconditioningtypeid','architecturalstyletypeid','basementsqft','buildingclasstypeid','buildingqualitytypeid'], inplace = True)\n",
    "df.drop(columns= ['calculatedbathnbr','decktypeid','finishedfloor1squarefeet','finishedsquarefeet12','finishedsquarefeet13','finishedsquarefeet15'], inplace = True)\n",
    "df.drop(columns= ['finishedsquarefeet50','finishedsquarefeet6','fips','fullbathcnt','heatingorsystemtypeid','poolsizesum','pooltypeid10','pooltypeid2'], inplace = True)\n",
    "df.drop(columns= ['pooltypeid7','propertycountylandusecode','propertyzoningdesc','rawcensustractandblock','regionidcity','regionidcounty','regionidneighborhood'], inplace = True)\n",
    "df.drop(columns= ['storytypeid','threequarterbathnbr','typeconstructiontypeid','unitcnt','yardbuildingsqft17','yardbuildingsqft26','numberofstories'], inplace = True)\n",
    "df.drop(columns= ['fireplaceflag','structuretaxvaluedollarcnt','assessmentyear','landtaxvaluedollarcnt','taxamount','taxdelinquencyflag','taxdelinquencyyear'], inplace = True)\n",
    "df.drop(columns= ['censustractandblock','logerror','transactiondate','garagetotalsqft'], inplace = True)\n",
    "\n",
    "# drop any nulls\n",
    "df = df.dropna()\n",
    "\n",
    "def get_object_cols(df):\n",
    "    '''\n",
    "    This function takes in a dataframe and identifies the columns that are object types\n",
    "    and returns a list of those column names. \n",
    "    '''\n",
    "    # create a mask of columns whether they are object type or not\n",
    "    mask = np.array(df.dtypes == \"object\")\n",
    "\n",
    "    # get a list of the column names that are objects (from the mask)\n",
    "    object_cols = df.iloc[:, mask].columns.tolist()\n",
    "    \n",
    "    return object_cols\n",
    "\n",
    "# get object column names\n",
    "object_cols = get_object_cols(df)\n",
    "\n",
    "def create_dummies(df, object_cols):\n",
    "    '''\n",
    "    This function takes in a dataframe and list of object column names,\n",
    "    and creates dummy variables of each of those columns. \n",
    "    It then appends the dummy variables to the original dataframe. \n",
    "    It returns the original df with the appended dummy variables. \n",
    "    '''\n",
    "    \n",
    "    # run pd.get_dummies() to create dummy vars for the object columns. \n",
    "    # we will drop the column representing the first unique value of each variable\n",
    "    # we will opt to not create na columns for each variable with missing values \n",
    "    # (all missing values have been removed.)\n",
    "    dummy_df = pd.get_dummies(object_cols, dummy_na=False, drop_first=True)\n",
    "    \n",
    "    # concatenate the dataframe with dummies to our original dataframe\n",
    "    # via column (axis=1)\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# create dummy vars\n",
    "df = create_dummies(df, object_cols)\n",
    "\n",
    "def train_validate_test(df, target):\n",
    "    '''\n",
    "    this function takes in a dataframe and splits it into 3 samples, \n",
    "    a test, which is 20% of the entire dataframe, \n",
    "    a validate, which is 24% of the entire dataframe,\n",
    "    and a train, which is 56% of the entire dataframe. \n",
    "    It then splits each of the 3 samples into a dataframe with independent variables\n",
    "    and a series with the dependent, or target variable. \n",
    "    The function returns 3 dataframes and 3 series:\n",
    "    X_train (df) & y_train (series), X_validate & y_validate, X_test & y_test. \n",
    "    '''\n",
    "    # split df into test (20%) and train_validate (80%)\n",
    "    train_validate, test = train_test_split(df, test_size=.2, random_state=123)\n",
    "\n",
    "    # split train_validate off into train (70% of 80% = 56%) and validate (30% of 80% = 24%)\n",
    "    train, validate = train_test_split(train_validate, test_size=.3, random_state=123)\n",
    "\n",
    "    # split train into X (dataframe, drop target) & y (series, keep target only)\n",
    "    X_train = train.drop(columns=[target])\n",
    "    y_train = train[target]\n",
    "    \n",
    "    # split validate into X (dataframe, drop target) & y (series, keep target only)\n",
    "    X_validate = validate.drop(columns=[target])\n",
    "    y_validate = validate[target]\n",
    "    \n",
    "    # split test into X (dataframe, drop target) & y (series, keep target only)\n",
    "    X_test = test.drop(columns=[target])\n",
    "    y_test = test[target]\n",
    "    \n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "\n",
    "# split data (taxvaluedollarcnt is target)\n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test = train_validate_test(df, 'taxvaluedollarcnt')\n",
    "\n",
    "def get_numeric_X_cols(X_train, object_cols):\n",
    "    '''\n",
    "    takes in a dataframe and list of object column names\n",
    "    and returns a list of all other columns names, the non-objects. \n",
    "    '''\n",
    "    numeric_cols = [col for col in X_train.columns.values if col not in object_cols]\n",
    "        \n",
    "    return numeric_cols\n",
    "\n",
    "# get numeric column names\n",
    "numeric_cols = get_numeric_X_cols(X_train, object_cols)\n",
    "\n",
    "# scale data \n",
    "#X_train_scaled, X_validate_scaled, X_test_scaled = min_max_scale(X_train, X_validate, X_test, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>fireplace</th>\n",
       "      <th>garage</th>\n",
       "      <th>hottub_spa</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>lotsizesquarefeet</th>\n",
       "      <th>poolcnt</th>\n",
       "      <th>propertylandusetypeid</th>\n",
       "      <th>regionidzip</th>\n",
       "      <th>roomcnt</th>\n",
       "      <th>yearbuilt</th>\n",
       "      <th>bedbathratio</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9014</th>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1695.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33520482.0</td>\n",
       "      <td>-117702935.0</td>\n",
       "      <td>3150.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>96987.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20816</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33770072.0</td>\n",
       "      <td>-117910110.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>97050.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13286</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33805800.0</td>\n",
       "      <td>-118307000.0</td>\n",
       "      <td>271197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>96210.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18128</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1864.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34227256.0</td>\n",
       "      <td>-119044607.0</td>\n",
       "      <td>7528.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>97089.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2491.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34231581.0</td>\n",
       "      <td>-118225941.0</td>\n",
       "      <td>21219.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>96271.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1974.0</td>\n",
       "      <td>1.33</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bathroomcnt  bedroomcnt  calculatedfinishedsquarefeet  fireplace  \\\n",
       "9014           2.5         3.0                        1695.0        0.0   \n",
       "20816          2.0         3.0                        1298.0        1.0   \n",
       "13286          2.0         3.0                        1277.0        0.0   \n",
       "18128          2.0         4.0                        1864.0        1.0   \n",
       "3629           3.0         4.0                        2491.0        0.0   \n",
       "\n",
       "       garage  hottub_spa    latitude    longitude  lotsizesquarefeet  \\\n",
       "9014      2.0         0.0  33520482.0 -117702935.0             3150.0   \n",
       "20816     2.0         0.0  33770072.0 -117910110.0             7200.0   \n",
       "13286     0.0         0.0  33805800.0 -118307000.0           271197.0   \n",
       "18128     2.0         0.0  34227256.0 -119044607.0             7528.0   \n",
       "3629      0.0         0.0  34231581.0 -118225941.0            21219.0   \n",
       "\n",
       "       poolcnt  propertylandusetypeid  regionidzip  roomcnt  yearbuilt  \\\n",
       "9014       1.0                  261.0      96987.0      7.0     1986.0   \n",
       "20816      0.0                  261.0      97050.0      6.0     1955.0   \n",
       "13286      0.0                  266.0      96210.0      0.0     1976.0   \n",
       "18128      0.0                  261.0      97089.0      2.0     1967.0   \n",
       "3629       1.0                  261.0      96271.0      0.0     1974.0   \n",
       "\n",
       "       bedbathratio  zip  \n",
       "9014           1.20  309  \n",
       "20816          1.50  340  \n",
       "13286          1.50  132  \n",
       "18128          2.00  355  \n",
       "3629           1.33  163  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bathroomcnt                     0\n",
       "bedroomcnt                      0\n",
       "calculatedfinishedsquarefeet    0\n",
       "fireplace                       0\n",
       "garage                          0\n",
       "hottub_spa                      0\n",
       "latitude                        0\n",
       "longitude                       0\n",
       "lotsizesquarefeet               0\n",
       "poolcnt                         0\n",
       "propertylandusetypeid           0\n",
       "regionidzip                     0\n",
       "roomcnt                         0\n",
       "yearbuilt                       0\n",
       "taxvaluedollarcnt               0\n",
       "bedbathratio                    0\n",
       "zip                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['bedbathratio'] = round(df['bedroomcnt'] / df['bathroomcnt'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e390343e6dcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX_validate_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrangle_zillow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX_train_explore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validate_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codeup-data-science/regression-exercises/regression_project/src/prepare.py\u001b[0m in \u001b[0;36mwrangle_zillow\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m# scale data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validate_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_max_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validate_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codeup-data-science/regression-exercises/regression_project/src/prepare.py\u001b[0m in \u001b[0;36mmin_max_scale\u001b[0;34m(X_train, X_validate, X_test, numeric_cols)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# if copy = false, inplace row normalization happens and avoids a copy (if the input is already a numpy array).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m#scale X_train, X_validate, X_test using the mins and maxes stored in the scaler derived from X_train.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_samples_seen_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         X = self._validate_data(X, reset=first_pass,\n\u001b[0m\u001b[1;32m    370\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                                 force_all_finite=\"allow-nan\")\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 )\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    646\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     96\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     98\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# path='zillow_df.csv'\n",
    "\n",
    "# df, X_train_explore, \\\n",
    "#     X_train_scaled, y_train, \\\n",
    "#     X_validate_scaled, y_validate, \\\n",
    "#     X_test_scaled, y_test = src.prepare.wrangle_zillow(path)\n",
    "\n",
    "# X_train_explore.shape, X_validate_scaled.shape, X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'fips'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-eb9aa9c75e1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#df.logerror.isnull().sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#test = df.roomcnt == df.bedroomcnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'fips'"
     ]
    }
   ],
   "source": [
    "#df.logerror.isnull().sum()\n",
    "#test = df.roomcnt == df.bedroomcnt \n",
    "df.fips.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_explore.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data\n",
    "\n",
    "*Make sure that your data is in a format that your models can work with, and fix any problems that need attention. This may include formatting strings, removing duplicate records, and handling missing data.*\n",
    "\n",
    "*There are multiple approaches to handling missing records, including removing them, inferring the values either statistically or from other insights, or finding other data sources to fill the gaps. Be sure to note down your thought process for whatever decision you make.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write functions that prepare the data from your data/raw and store\n",
    "# the prepared data in data/\n",
    "# Once that is done move the functions to prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "\n",
    "**If you haven't split the data yet, do it before you explore**\n",
    "\n",
    "*Before you start exploring the data, write out your thought process about what you're looking for and what you expect to find. Take a minute to confirm that your plan actually makes sense.*\n",
    "\n",
    "*Calculate summary statistics and plot some charts to give you an idea what types of useful relationships might be in your dataset. Use these insights to go back and download additional data or engineer new features if necessary. Not now though... remember we're still just trying to finish the MVP!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Distributions\n",
    "\n",
    "*It's good to have a sense of things like the scale and the spread of the data you're working with. But looking at the distribution of each feature can be a helpful way to determine if more features are needed.*\n",
    "\n",
    "*Look for outliers. Take a closer look at the observations that stand out from the rest. Is there anything about these data points that might explain why they aren't behaving like the others? If so, that's a feature you would want to add to your model, to control for such a big difference.*\n",
    "\n",
    "*Look for clusters in your histograms and scatterplots of various features. If the variance is inconsistent, with dense clusters of observations within the feature space, try to figure out what makes those observations so much more similar than the others. This could point to another feature to add to your data. Investigate any unusual patterns that look like a feature is not being drawn from a random distribution.*\n",
    "\n",
    "*Look for multiple modes within the distributions of your features. This is a sign that there are categorical variables within your data set. Try to find out what these classes are, if you don't already know, and label your observations if possible.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Relationships\n",
    "\n",
    "*The \"eyeball test\" is the quickest way to check that your hypothesis is on the right track. Plot your inputs against your outputs. If there is real predictive power for the model you are trying to build, you should be able to make reasonable guesses (just by pointing at the chart, no need for precise values yet) of what the output should be for any given input value.*\n",
    "\n",
    "*In a regression model, we are looking for clear relationships between our features and targets. Generate pairplots and look for features that have high correlations with the target variable.*\n",
    "\n",
    "*In a classification model, we are looking for features that separate the population into distinct distributions. Generate pairplots that are color-coded by your categories and look for features where the categories have distinct distributions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model the Data\n",
    "\n",
    "*Describe the algorithms that you are considering. How do they work? Why are they good choices for this data and problem space?*\n",
    "\n",
    "*What nuances in the data will you have to be aware of in order to avoid introducing bias to your model? What steps will you need to take to prevent overfitting? What risks are there for data leakage?*\n",
    "\n",
    "### Train Validation Test Split\n",
    "\n",
    "*Pay special attention here to what data is going into your training and test sets. Is there any data leakage? Make sure that you are testing your model on information it has not seen during the learning process. If this is a classification problem, make sure that your classes are reasonably balanced.*\n",
    "\n",
    "*Also, keep in mind that after you've trained and evaluated your model, you may want to look at the results of specific missed predictions. Set up your training and test data sets so that it will be easy to identify the record id when you are looking at a given prediction result.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "*These are standard processing steps to prepare your data for modeling. But remember that any code you use to scale or encode your data has to come from the observations in your training set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train Model\n",
    "\n",
    "*Write down any thoughts you may have about working with these algorithms on this data. What looks to have been the most successful design choices? What pain points are you running into? What other ideas do you want to try out as you iterate on this pipeline?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and Score\n",
    "\n",
    "Important: Stick to the metrics you chose to test before you trained and evaluated your model! Remember that this is the standard you selected before your analysis as the one that you would find most convincing that the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret the Model\n",
    "\n",
    "Write up the things you learned, and how well your model performed. Be sure address the model's strengths and weaknesses. What types of data does it handle well? What types of observations tend to give it a hard time? What future work would you or someone reading this might want to do, building on the lessons learned and tools developed in this project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report Metrics in Context\n",
    "How did the model perform on the key metrics you chose to demonstrate its usefulness? What counts as good or bad performance? How well do humans perform on this task? How well would you expect to do with random dice rolls? What are the costs associated with missed predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Errors\n",
    "\n",
    "Look at some of the observations with missed predictions. (Do this in your validation set, never look at individual records in your test set!) Are there common patterns among the observations with bad results? Do you have data that you can include in your model that will capture these patterns or is this a task that will need another research project to solve?\n",
    "\n",
    "This is your last step in the iteration cycle; if you can't find anything else you can work on here with your present data, project scope, and deadlines, then it's time to wrap things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strengths and Weaknesses\n",
    "\n",
    "Once you've gone through your iteration cycles and are finished with this version of the model, or this particular project, provide an assessment of what types of observations are handled well by your model, and what circumstances seem to give it trouble. This will point you and others towards more questions for future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps: What Can We Do Now?\n",
    "\n",
    "Reporting a model's results is good, and is the main objective of any data science project. But a project is one thing, a career is another. A question is one thing, but science is another. If you've carried out your research with a mindset of curiosity and creativity, then by now you should have plenty more, and much better informed, questions about this topic than what you started with.\n",
    "\n",
    "So in addition to reporting on the question you investigated and the answers you found, think of the needs of your team, your users, and your peers in the industry, and make some recommendations that answer these two questions:\n",
    "\n",
    "What are some unanswered questions in my project where more information (additional data sources, deeper understanding, other models or tools) might help improve these results?\n",
    "What are other needs or problems where my model or my approach may be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
